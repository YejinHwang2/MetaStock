{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e38ca4a8",
   "metadata": {},
   "source": [
    "## Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd72dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb0f843",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723b0eda",
   "metadata": {},
   "source": [
    "### using args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2170afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaStockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,args):\n",
    "        super(MetaStockDataset,self).__init__()\n",
    "        self.args = args\n",
    "        self.no_cuda = args.no_cuda\n",
    "        self.data_path =  args.data_path\n",
    "#         self.date_format = \"%Y-%m-%d\" \n",
    "        self.verbose = args.verbose\n",
    "        self.fnames = [fname for fname in os.listdir(self.data_path) if\n",
    "              os.path.isfile(os.path.join(self.data_path, fname))]\n",
    "        self.data_EOD = []\n",
    "        \n",
    "        for index, fname in enumerate(self.fnames):\n",
    "            # print(fname)\n",
    "            single_EOD = np.genfromtxt(\n",
    "                os.path.join(self.data_path, fname), dtype=float, delimiter=',',\n",
    "                skip_header=False\n",
    "            )\n",
    "            # print('data shape:', single_EOD.shape)\n",
    "            self.data_EOD.append(single_EOD)\n",
    "        \n",
    "        self.trading_dates = np.genfromtxt(\n",
    "            os.path.join(self.data_path, '..', 'trading_dates.csv'), dtype=str,\n",
    "            delimiter=',', skip_header=False)\n",
    "        self.windows= [5, 10, 15, 20] \n",
    "        random.seed(self.args.seed)\n",
    "        self.train_stock_num = args.train_stock_num\n",
    "        self.test_stock_num = len(self.fnames) - self.train_stock_num\n",
    "        self.train_date = args.train_date\n",
    "        self.test_date = args.test_date\n",
    "        self.train_stock = []\n",
    "        \n",
    "        \"\"\"\n",
    "        dataset ref: https://arxiv.org/abs/1810.09936\n",
    "        In this meta learning setting, we have 3 meta-test and 1 meta-train\n",
    "        vertical = stocks, horizontal = time\n",
    "                train      |    test\n",
    "           A               |\n",
    "           B   meta-train  |   meta-test\n",
    "           C               |      (1)\n",
    "           ----------------|-------------\n",
    "           D   meta-test   |   meta-test\n",
    "           E     (2)       |      (3)\n",
    "\n",
    "        meta-test (1) same stock, different time\n",
    "        meta-test (2) different stock, same time\n",
    "        meta-test (3) different stock, different time\n",
    "        use `valid_date` to split the train / test set\n",
    "        \n",
    "        \"\"\"\n",
    "        def split_data(train_date, test_date):\n",
    "            # train_date: start date for meta-train dataset\n",
    "            # test_date: start date for meta-test dataset\n",
    "\n",
    "            dates_index = {}\n",
    "            for index, date in enumerate(self.trading_dates):\n",
    "                dates_index[date] = index\n",
    "\n",
    "            train_ind = dates_index[train_date]\n",
    "            test_ind = dates_index[test_date]\n",
    "            return train_ind, test_ind \n",
    "\n",
    "        self.train_ind, self.test_ind = split_data(self.train_date, self.test_date)\n",
    "        if args.train == 'train':\n",
    "            self.metasplit = ['train']\n",
    "\n",
    "        elif args.train == 'test1':\n",
    "            self.metasplit = ['test1']\n",
    "            self.train_stock = args.train_stock\n",
    "        elif args.train == 'test2':\n",
    "            self.metasplit = ['test2']\n",
    "\n",
    "        elif args.train == 'test3':\n",
    "            self.metasplit = ['test3']\n",
    "\n",
    "        assert args.train in ['train', 'test1','test2','test3']\n",
    "\n",
    "    def task_formation(self,task_num):\n",
    "        stock = []\n",
    "        support = [[],[]]\n",
    "        support_idx = [[],[]]\n",
    "        support_stock = []\n",
    "        query = [[],[]]\n",
    "        query_idx =[[],[]]\n",
    "        \n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            if self.metasplit == ['train']:\n",
    "                tick_ind = random.randint(0,self.train_stock_num)\n",
    "                stock.append(tick_ind)\n",
    "            elif self.metasplit == ['test1']:\n",
    "                tick_ind = random.choice(self.train_stock)\n",
    "                stock.append(tick_ind)\n",
    "\n",
    "            elif self.metasplit == ['test2'] or self.metasplit == ['test3']:\n",
    "                tick_ind = random.randint(self.train_stock_num, len(self.fnames)-1)\n",
    "                stock.append(tick_ind)\n",
    "\n",
    "            fname = self.fnames[tick_ind]\n",
    "            single_EOD = np.genfromtxt(os.path.join(self.data_path, fname), dtype=float, delimiter=',',skip_header=False)\n",
    "\n",
    "            def target_date_selection(window):\n",
    "                if self.metasplit == ['train'] or self.metasplit == ['test2']:\n",
    "                    target_date_ind = random.randint(self.train_ind, self.test_ind)\n",
    "                if self.metasplit == ['test1'] or self.metasplit == ['test3']:\n",
    "                    target_date_ind = random.randint(self.test_ind, len(self.trading_dates)-2)\n",
    "                while target_date_ind <= window or abs(single_EOD[target_date_ind][-2]) < 1e-8 or \\\n",
    "                abs(single_EOD[target_date_ind+1][-2]) < 1e-8 or single_EOD[target_date_ind - window: target_date_ind, :].min() < -123320:\n",
    "                    if self.metasplit == ['train'] or self.metasplit == ['test2']:\n",
    "                        target_date_ind = random.randint(self.train_ind, self.test_ind)\n",
    "                    if self.metasplit == ['test1'] or self.metasplit == ['test3']:\n",
    "                        target_date_ind = random.randint(self.test_ind, len(self.trading_dates)-2)\n",
    "\n",
    "                return target_date_ind\n",
    "\n",
    "            for window in self.windows:\n",
    "                target_date_ind = target_date_selection(window)\n",
    "                x_support_idx = np.array(range(target_date_ind-window, target_date_ind))\n",
    "                y_support_idx = target_date_ind\n",
    "                support_idx[0].append(x_support_idx)\n",
    "                support_idx[1].append(y_support_idx)\n",
    "                support[0].append(single_EOD[x_support_idx, :11])\n",
    "                support[1].append((single_EOD[y_support_idx][-2]+1)/2)\n",
    "\n",
    "                x_query_idx = np.array(range(target_date_ind-window, target_date_ind+1))\n",
    "                y_query_idx = target_date_ind+1\n",
    "                query_idx[0].append(x_query_idx)\n",
    "                query_idx[1].append(y_query_idx)\n",
    "                query[0].append(single_EOD[x_query_idx, :11])\n",
    "                query[1].append((single_EOD[y_query_idx][-2]+1)/2)\n",
    "\n",
    "\n",
    "        self.train_stock = stock\n",
    "        return stock, support_idx, support, query_idx, query      \n",
    "\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a2ad97",
   "metadata": {},
   "source": [
    "## using config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cb3b113a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Config: Dict\n",
    "# {'no_cuda': bool, 'data_path': str, 'verbose': int, 'seed':int, 'train_stock_num':int,\n",
    "# 'train_date': str, 'test_date': str, 'train':str}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d19b540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetaStockDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,config):\n",
    "        super(MetaStockDataset,self).__init__()\n",
    "        self.config = config\n",
    "        self.no_cuda = config['no_cuda']\n",
    "        self.data_path =  config['data_path']\n",
    "#         self.date_format = \"%Y-%m-%d\" \n",
    "        self.verbose = config['verbose']\n",
    "        self.fnames = [fname for fname in os.listdir(self.data_path) if\n",
    "              os.path.isfile(os.path.join(self.data_path, fname))]\n",
    "        self.data_EOD = []\n",
    "        \n",
    "        for index, fname in enumerate(self.fnames):\n",
    "            # print(fname)\n",
    "            single_EOD = np.genfromtxt(\n",
    "                os.path.join(self.data_path, fname), dtype=float, delimiter=',',\n",
    "                skip_header=False\n",
    "            )\n",
    "            # print('data shape:', single_EOD.shape)\n",
    "            self.data_EOD.append(single_EOD)\n",
    "        \n",
    "        self.trading_dates = np.genfromtxt(\n",
    "            os.path.join(self.data_path, '..', 'trading_dates.csv'), dtype=str,\n",
    "            delimiter=',', skip_header=False)\n",
    "        self.windows= [5, 10, 15, 20] \n",
    "        random.seed(self.config['seed'])\n",
    "        self.train_stock_num = config['train_stock_num']\n",
    "        self.test_stock_num = len(self.fnames) - self.train_stock_num\n",
    "        self.train_date = config['train_date']\n",
    "        self.test_date = config['test_date']\n",
    "        self.train_stock = []\n",
    "        \n",
    "        \"\"\"\n",
    "        dataset ref: https://arxiv.org/abs/1810.09936\n",
    "        In this meta learning setting, we have 3 meta-test and 1 meta-train\n",
    "        vertical = stocks, horizontal = time\n",
    "                train      |    test\n",
    "           A               |\n",
    "           B   meta-train  |   meta-test\n",
    "           C               |      (1)\n",
    "           ----------------|-------------\n",
    "           D   meta-test   |   meta-test\n",
    "           E     (2)       |      (3)\n",
    "\n",
    "        meta-test (1) same stock, different time\n",
    "        meta-test (2) different stock, same time\n",
    "        meta-test (3) different stock, different time\n",
    "        use `valid_date` to split the train / test set\n",
    "        \n",
    "        \"\"\"\n",
    "        def split_data(train_date, test_date):\n",
    "            # train_date: start date for meta-train dataset\n",
    "            # test_date: start date for meta-test dataset\n",
    "\n",
    "            dates_index = {}\n",
    "            for index, date in enumerate(self.trading_dates):\n",
    "                dates_index[date] = index\n",
    "\n",
    "            train_ind = dates_index[train_date]\n",
    "            test_ind = dates_index[test_date]\n",
    "            return train_ind, test_ind \n",
    "\n",
    "        self.train_ind, self.test_ind = split_data(self.train_date, self.test_date)\n",
    "        if config['train'] == 'train':\n",
    "            self.metasplit = ['train']\n",
    "\n",
    "        elif config['train'] == 'test1':\n",
    "            self.metasplit = ['test1']\n",
    "            self.train_stock = config['train_stock']\n",
    "        elif config['train'] == 'test2':\n",
    "            self.metasplit = ['test2']\n",
    "\n",
    "        elif config['train'] == 'test3':\n",
    "            self.metasplit = ['test3']\n",
    "\n",
    "        assert config['train'] in ['train', 'test1','test2','test3']\n",
    "\n",
    "    def task_formation(self,task_num):\n",
    "        stock = []\n",
    "        support = [[],[]]\n",
    "        support_idx = [[],[]]\n",
    "        support_stock = []\n",
    "        query = [[],[]]\n",
    "        query_idx =[[],[]]\n",
    "        \n",
    "\n",
    "        for i in range(task_num):\n",
    "\n",
    "            if self.metasplit == ['train']:\n",
    "                tick_ind = random.randint(0,self.train_stock_num)\n",
    "                stock.append(tick_ind)\n",
    "            elif self.metasplit == ['test1']:\n",
    "                tick_ind = random.choice(self.train_stock)\n",
    "                stock.append(tick_ind)\n",
    "\n",
    "            elif self.metasplit == ['test2'] or self.metasplit == ['test3']:\n",
    "                tick_ind = random.randint(self.train_stock_num, len(self.fnames)-1)\n",
    "                stock.append(tick_ind)\n",
    "\n",
    "            fname = self.fnames[tick_ind]\n",
    "            single_EOD = np.genfromtxt(os.path.join(self.data_path, fname), dtype=float, delimiter=',',skip_header=False)\n",
    "\n",
    "            def target_date_selection(window):\n",
    "                if self.metasplit == ['train'] or self.metasplit == ['test2']:\n",
    "                    target_date_ind = random.randint(self.train_ind, self.test_ind)\n",
    "                if self.metasplit == ['test1'] or self.metasplit == ['test3']:\n",
    "                    target_date_ind = random.randint(self.test_ind, len(self.trading_dates)-2)\n",
    "                while target_date_ind <= window or abs(single_EOD[target_date_ind][-2]) < 1e-8 or \\\n",
    "                abs(single_EOD[target_date_ind+1][-2]) < 1e-8 or single_EOD[target_date_ind - window: target_date_ind, :].min() < -123320:\n",
    "                    if self.metasplit == ['train'] or self.metasplit == ['test2']:\n",
    "                        target_date_ind = random.randint(self.train_ind, self.test_ind)\n",
    "                    if self.metasplit == ['test1'] or self.metasplit == ['test3']:\n",
    "                        target_date_ind = random.randint(self.test_ind, len(self.trading_dates)-2)\n",
    "\n",
    "                return target_date_ind\n",
    "\n",
    "            for window in self.windows:\n",
    "                target_date_ind = target_date_selection(window)\n",
    "                x_support_idx = np.array(range(target_date_ind-window, target_date_ind))\n",
    "                y_support_idx = target_date_ind\n",
    "                support_idx[0].append(x_support_idx)\n",
    "                support_idx[1].append(y_support_idx)\n",
    "                support[0].append(single_EOD[x_support_idx, :11])\n",
    "                support[1].append((single_EOD[y_support_idx][-2]+1)/2)\n",
    "\n",
    "                x_query_idx = np.array(range(target_date_ind-window, target_date_ind+1))\n",
    "                y_query_idx = target_date_ind+1\n",
    "                query_idx[0].append(x_query_idx)\n",
    "                query_idx[1].append(y_query_idx)\n",
    "                query[0].append(single_EOD[x_query_idx, :11])\n",
    "                query[1].append((single_EOD[y_query_idx][-2]+1)/2)\n",
    "\n",
    "\n",
    "        self.train_stock = stock\n",
    "        return stock, support_idx, support, query_idx, query      \n",
    "\n",
    "                \n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b2b653",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429105e",
   "metadata": {},
   "source": [
    "### Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e848d9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', default='/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', type=str)\n",
    "parser.add_argument('--no_cuda', default='False', type=bool)\n",
    "parser.add_argument('--verbose', default=0, type=int)\n",
    "parser.add_argument('--seed', default=300, type=int)\n",
    "parser.add_argument('--train_stock_num', default=67, type=int)\n",
    "parser.add_argument('--train_date', default='2014-01-02', type=str)\n",
    "parser.add_argument('--test_date', default='2015-08-03', type=str)\n",
    "parser.add_argument('--train', default='train', type=str)\n",
    "parser.add_argument('--train_stock', default= [], type=list)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef37723e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {'no_cuda': False, 'data_path': '/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', 'verbose': 0, 'seed': 300, 'train_stock_num':67,\n",
    "'train_date': '2014-01-02', 'test_date': '2015-08-03', 'train':'train', 'train_stock': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "51013826",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MetaStockDataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f282dc50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(data_path='/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', no_cuda=True, seed=300, test_date='2015-08-03', train='train', train_date='2014-01-02', train_stock=[], train_stock_num=67, verbose=0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ea325f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = MetaStockDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa753c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stock, train_support_idx,train_support, train_query_idx, train_query = train_data.task_formation(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbb7551e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_support_idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94f5c823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7209e-01,  4.7941e-01, -1.7209e-01,  1.6494e+00,  1.6494e+00,\n",
       "         -3.4420e-02,  8.0762e-01,  2.1955e+00,  2.9607e+00,  4.1711e+00,\n",
       "          4.0746e+00],\n",
       "        [-4.9168e-02,  7.9912e-01, -7.3764e-01, -1.2295e-02, -1.2298e-02,\n",
       "         -5.6798e-01,  7.1675e-01,  1.9974e+00,  2.6856e+00,  3.7507e+00,\n",
       "          4.0419e+00],\n",
       "        [-4.0010e-01,  6.0616e-02, -1.0063e+00,  1.4015e+00,  1.4016e+00,\n",
       "         -1.5762e+00, -6.4624e-01,  4.5180e-01,  9.8082e-01,  1.9912e+00,\n",
       "          2.6495e+00],\n",
       "        [-2.7060e-01,  8.4870e-01, -3.5670e-01, -1.4306e+00, -1.4307e+00,\n",
       "         -2.0000e-06,  5.0061e-01,  1.5055e+00,  2.1402e+00,  3.0534e+00,\n",
       "          4.1103e+00],\n",
       "        [-8.7623e-01,  3.4076e-01, -8.8840e-01,  1.0701e+00,  1.0701e+00,\n",
       "         -5.3790e-01, -6.9612e-01,  1.0872e-01,  9.1092e-01,  1.7388e+00,\n",
       "          2.8652e+00]], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(train_support[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c1733b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "96b989e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_support[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d013f008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 45\n",
      "x_support index: [143 144 145 146 147]\n",
      "y_support index: 148\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in support set\n",
    "print(\"Tick index:\", train_stock[0])\n",
    "print(\"x_support index:\", train_support_idx[0][0])\n",
    "print(\"y_support index:\", train_support_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d756804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_support:\n",
      " [[-1.720950e-01  4.794100e-01 -1.720950e-01  1.649380e+00  1.649370e+00\n",
      "  -3.442000e-02  8.076230e-01  2.195455e+00  2.960668e+00  4.171117e+00\n",
      "   4.074581e+00]\n",
      " [-4.916800e-02  7.991170e-01 -7.376420e-01 -1.229500e-02 -1.229800e-02\n",
      "  -5.679810e-01  7.167500e-01  1.997384e+00  2.685650e+00  3.750686e+00\n",
      "   4.041893e+00]\n",
      " [-4.000980e-01  6.061600e-02 -1.006306e+00  1.401533e+00  1.401559e+00\n",
      "  -1.576161e+00 -6.462440e-01  4.518010e-01  9.808200e-01  1.991246e+00\n",
      "   2.649508e+00]\n",
      " [-2.706040e-01  8.487020e-01 -3.567050e-01 -1.430650e+00 -1.430667e+00\n",
      "  -2.000000e-06  5.006070e-01  1.505527e+00  2.140215e+00  3.053376e+00\n",
      "   4.110285e+00]\n",
      " [-8.762330e-01  3.407560e-01 -8.883970e-01  1.070105e+00  1.070102e+00\n",
      "  -5.379040e-01 -6.961170e-01  1.087190e-01  9.109190e-01  1.738837e+00\n",
      "   2.865202e+00]]\n",
      "y_support:\n",
      " 0.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in support set\n",
    "print(\"x_support:\\n\", train_support[0][0])\n",
    "print(\"y_support:\\n\", train_support[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "32c8dee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 45\n",
      "x_support index: [143 144 145 146 147 148]\n",
      "y_support index: 149\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in query set\n",
    "print(\"Tick index:\", train_stock[0])\n",
    "print(\"x_support index:\", train_query_idx[0][0])\n",
    "print(\"y_support index:\", train_query_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "14c2ba7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_query:\n",
      " [[-1.720950e-01  4.794100e-01 -1.720950e-01  1.649380e+00  1.649370e+00\n",
      "  -3.442000e-02  8.076230e-01  2.195455e+00  2.960668e+00  4.171117e+00\n",
      "   4.074581e+00]\n",
      " [-4.916800e-02  7.991170e-01 -7.376420e-01 -1.229500e-02 -1.229800e-02\n",
      "  -5.679810e-01  7.167500e-01  1.997384e+00  2.685650e+00  3.750686e+00\n",
      "   4.041893e+00]\n",
      " [-4.000980e-01  6.061600e-02 -1.006306e+00  1.401533e+00  1.401559e+00\n",
      "  -1.576161e+00 -6.462440e-01  4.518010e-01  9.808200e-01  1.991246e+00\n",
      "   2.649508e+00]\n",
      " [-2.706040e-01  8.487020e-01 -3.567050e-01 -1.430650e+00 -1.430667e+00\n",
      "  -2.000000e-06  5.006070e-01  1.505527e+00  2.140215e+00  3.053376e+00\n",
      "   4.110285e+00]\n",
      " [-8.762330e-01  3.407560e-01 -8.883970e-01  1.070105e+00  1.070102e+00\n",
      "  -5.379040e-01 -6.961170e-01  1.087190e-01  9.109190e-01  1.738837e+00\n",
      "   2.865202e+00]\n",
      " [ 1.438761e+00  1.438761e+00 -2.126840e-01 -2.726053e+00 -2.726057e+00\n",
      "   1.894164e+00  1.817844e+00  2.363738e+00  3.481800e+00  4.210689e+00\n",
      "   5.333838e+00]]\n",
      "y_query:\n",
      " -1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in query set\n",
    "print(\"x_query:\\n\", train_query[0][0])\n",
    "print(\"y_query:\\n\", train_query[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3d3e952",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_stock = train_data.train_stock"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156ff552",
   "metadata": {},
   "source": [
    "### Test1 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "190877a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', default='/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', type=str)\n",
    "parser.add_argument('--no_cuda', default='False', type=bool)\n",
    "parser.add_argument('--verbose', default=0, type=int)\n",
    "parser.add_argument('--seed', default=200, type=int)\n",
    "parser.add_argument('--train_stock_num', default=67, type=int)\n",
    "parser.add_argument('--train_date', default='2014-01-02', type=str)\n",
    "parser.add_argument('--test_date', default='2015-08-03', type=str)\n",
    "parser.add_argument('--train', default='test1', type=str)\n",
    "parser.add_argument('--train_stock', default= train_stock, type=list)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "53b6e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_data = MetaStockDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dae8d5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_stock, test1_support_idx, test1_support, test1_query_idx, test1_query = test1_data.task_formation(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d70c3fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 45\n",
      "x_support index: [606 607 608 609 610]\n",
      "y_support index: 611\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in support set\n",
    "print(\"Tick index:\", test1_stock[0])\n",
    "print(\"x_support index:\", test1_support_idx[0][0])\n",
    "print(\"y_support index:\", test1_support_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b1be1e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_support:\n",
      " [[ 0.041038  0.30096  -0.670311 -0.381574 -0.381583  0.703148  0.604651\n",
      "  -0.722299 -3.630643 -6.060191 -6.4259  ]\n",
      " [-0.517355  0.966654 -1.824365  0.478795  0.478789  0.487413  0.343096\n",
      "  -0.890395 -3.279095 -5.957788 -6.669382]\n",
      " [ 0.124562  0.429063 -1.051906 -1.63376  -1.633758  1.749481  1.799307\n",
      "   0.858131 -0.968166 -3.862699 -4.937946]\n",
      " [-0.596559  0.735293 -1.040511 -0.235291 -0.235291  1.07103   1.789675\n",
      "   1.28838  -0.172035 -3.069371 -4.429803]\n",
      " [-1.091765  0.386951 -1.616914  0.388456  0.388455  0.398009  1.18021\n",
      "   1.012529 -0.126452 -2.795468 -4.528746]]\n",
      "y_support:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in support set\n",
    "print(\"x_support:\\n\", test1_support[0][0])\n",
    "print(\"y_support:\\n\", test1_support[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fd91946d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 45\n",
      "x_query index: [606 607 608 609 610 611]\n",
      "y_query index: 612\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in query set\n",
    "print(\"Tick index:\", test1_stock[0])\n",
    "print(\"x_query index:\", test1_query_idx[0][0])\n",
    "print(\"y_query index:\", test1_query_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5c8bd165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_query:\n",
      " [[ 0.041038  0.30096  -0.670311 -0.381574 -0.381583  0.703148  0.604651\n",
      "  -0.722299 -3.630643 -6.060191 -6.4259  ]\n",
      " [-0.517355  0.966654 -1.824365  0.478795  0.478789  0.487413  0.343096\n",
      "  -0.890395 -3.279095 -5.957788 -6.669382]\n",
      " [ 0.124562  0.429063 -1.051906 -1.63376  -1.633758  1.749481  1.799307\n",
      "   0.858131 -0.968166 -3.862699 -4.937946]\n",
      " [-0.596559  0.735293 -1.040511 -0.235291 -0.235291  1.07103   1.789675\n",
      "   1.28838  -0.172035 -3.069371 -4.429803]\n",
      " [-1.091765  0.386951 -1.616914  0.388456  0.388455  0.398009  1.18021\n",
      "   1.012529 -0.126452 -2.795468 -4.528746]\n",
      " [-1.616632  0.801516 -1.643797  1.727474  1.727472 -1.168319 -0.581441\n",
      "  -0.451025 -1.34968  -3.672326 -5.787256]]\n",
      "y_query:\n",
      " -1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in query set\n",
    "print(\"x_query:\\n\", test1_query[0][0])\n",
    "print(\"y_query:\\n\", test1_query[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87eedb",
   "metadata": {},
   "source": [
    "### Test2 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "18a6c4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', default='/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', type=str)\n",
    "parser.add_argument('--no_cuda', default='False', type=bool)\n",
    "parser.add_argument('--verbose', default=0, type=int)\n",
    "parser.add_argument('--seed', default=200, type=int)\n",
    "parser.add_argument('--train_stock_num', default=67, type=int)\n",
    "parser.add_argument('--train_date', default='2014-01-02', type=str)\n",
    "parser.add_argument('--test_date', default='2015-08-03', type=str)\n",
    "parser.add_argument('--train', default='test2', type=str)\n",
    "parser.add_argument('--train_stock', default= [], type=list)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5caa7853",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_data = MetaStockDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9fb49529",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2_stock, test2_support_idx, test2_support, test2_query_idx, test2_query = test2_data.task_formation(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d1373a29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 68\n",
      "x_support index: [519 520 521 522 523]\n",
      "y_support index: 524\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in support set\n",
    "print(\"Tick index:\", test2_stock[0])\n",
    "print(\"x_support index:\", test2_support_idx[0][0])\n",
    "print(\"y_support index:\", test2_support_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a91e848b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_support:\n",
      " [[ 0.247991  0.7794   -0.023614 -0.458446 -0.458429  0.521958  0.123988\n",
      "   0.062185  0.172404  0.512035  0.872293]\n",
      " [ 1.096149  1.096149  0.       -0.885687 -0.88569   1.050872  0.880492\n",
      "   0.933312  0.997253  1.257708  1.669241]\n",
      " [-0.035773  0.286188 -0.441215 -0.083402 -0.083399  0.813254  0.945618\n",
      "   0.984172  1.001659  1.182437  1.630484]\n",
      " [ 0.277714  0.760682 -0.024145 -1.240163 -1.240168  1.511712  2.104566\n",
      "   2.088866  2.130519  2.274326  2.724378]\n",
      " [ 0.64904   2.043275 -0.480762  0.458823  0.458807  0.598576  1.460355\n",
      "   1.512036  1.54749   1.70386   2.108989]]\n",
      "y_support:\n",
      " -1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in support set\n",
    "print(\"x_support:\\n\", test2_support[0][0])\n",
    "print(\"y_support:\\n\", test2_support[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "57bd9782",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 68\n",
      "x_query index: [519 520 521 522 523 524]\n",
      "y_query index: 525\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in query set\n",
    "print(\"Tick index:\", test2_stock[0])\n",
    "print(\"x_query index:\", test2_query_idx[0][0])\n",
    "print(\"y_query index:\", test2_query_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bcaed633",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_query:\n",
      " [[ 0.247991  0.7794   -0.023614 -0.458446 -0.458429  0.521958  0.123988\n",
      "   0.062185  0.172404  0.512035  0.872293]\n",
      " [ 1.096149  1.096149  0.       -0.885687 -0.88569   1.050872  0.880492\n",
      "   0.933312  0.997253  1.257708  1.669241]\n",
      " [-0.035773  0.286188 -0.441215 -0.083402 -0.083399  0.813254  0.945618\n",
      "   0.984172  1.001659  1.182437  1.630484]\n",
      " [ 0.277714  0.760682 -0.024145 -1.240163 -1.240168  1.511712  2.104566\n",
      "   2.088866  2.130519  2.274326  2.724378]\n",
      " [ 0.64904   2.043275 -0.480762  0.458823  0.458807  0.598576  1.460355\n",
      "   1.512036  1.54749   1.70386   2.108989]\n",
      " [ 1.104766  1.408275 -0.315651 -0.997589 -0.997585  1.051357  2.196193\n",
      "   2.305051  2.41411   2.595608  2.950915]]\n",
      "y_query:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in query set\n",
    "print(\"x_query:\\n\", test2_query[0][0])\n",
    "print(\"y_query:\\n\", test2_query[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6142ceb",
   "metadata": {},
   "source": [
    "### Test3 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fdb314e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data_path', default='/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', type=str)\n",
    "parser.add_argument('--no_cuda', default='False', type=bool)\n",
    "parser.add_argument('--verbose', default=0, type=int)\n",
    "parser.add_argument('--seed', default=150, type=int)\n",
    "parser.add_argument('--train_stock_num', default=67, type=int)\n",
    "parser.add_argument('--train_date', default='2014-01-02', type=str)\n",
    "parser.add_argument('--test_date', default='2015-08-03', type=str)\n",
    "parser.add_argument('--train', default='test3', type=str)\n",
    "parser.add_argument('--train_stock', default= [], type=list)\n",
    "args = parser.parse_args(args=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "00aab2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3_data = MetaStockDataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "cf092368",
   "metadata": {},
   "outputs": [],
   "source": [
    "test3_stock, test3_support_idx, test3_support, test3_query_idx, test3_query = test3_data.task_formation(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "42e25f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 77\n",
      "x_support index: [592 593 594 595 596]\n",
      "y_support index: 597\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in support set\n",
    "print(\"Tick index:\", test3_stock[0])\n",
    "print(\"x_support index:\", test3_support_idx[0][0])\n",
    "print(\"y_support index:\", test3_support_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b1270adc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_support:\n",
      " [[ 0.311769  1.032732 -0.837882  0.509203  0.509199 -1.282148 -2.449333\n",
      "  -2.081056 -1.449723 -1.155101 -0.519612]\n",
      " [-1.084225  0.367857 -1.742501  0.643028  0.643034 -1.23137  -2.58471\n",
      "  -2.706685 -2.023238 -1.79439  -1.195875]\n",
      " [ 0.856362  1.245619 -0.875829 -0.522751 -0.522759 -0.128451 -1.911246\n",
      "  -2.053973 -1.481116 -1.310233 -0.810299]\n",
      " [ 0.15601   0.409522 -0.780027 -0.194632 -0.194625  0.113105 -1.2812\n",
      "  -1.78887  -1.259748 -1.063961 -0.746879]\n",
      " [-0.70299   0.976372 -0.742037 -0.136505 -0.136515  0.308541 -0.714694\n",
      "  -1.546564 -1.201903 -0.963083 -0.758956]]\n",
      "y_support:\n",
      " -1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in support set\n",
    "print(\"x_support:\\n\", test3_support[0][0])\n",
    "print(\"y_support:\\n\", test3_support[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "3771c879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tick index: 68\n",
      "x_query index: [592 593 594 595 596 597]\n",
      "y_query index: 598\n"
     ]
    }
   ],
   "source": [
    "# tick index & data sample index of the first task in query set\n",
    "print(\"Tick index:\", test2_stock[0])\n",
    "print(\"x_query index:\", test3_query_idx[0][0])\n",
    "print(\"y_query index:\", test3_query_idx[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cdf46c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_query:\n",
      " [[ 0.311769  1.032732 -0.837882  0.509203  0.509199 -1.282148 -2.449333\n",
      "  -2.081056 -1.449723 -1.155101 -0.519612]\n",
      " [-1.084225  0.367857 -1.742501  0.643028  0.643034 -1.23137  -2.58471\n",
      "  -2.706685 -2.023238 -1.79439  -1.195875]\n",
      " [ 0.856362  1.245619 -0.875829 -0.522751 -0.522759 -0.128451 -1.911246\n",
      "  -2.053973 -1.481116 -1.310233 -0.810299]\n",
      " [ 0.15601   0.409522 -0.780027 -0.194632 -0.194625  0.113105 -1.2812\n",
      "  -1.78887  -1.259748 -1.063961 -0.746879]\n",
      " [-0.70299   0.976372 -0.742037 -0.136505 -0.136515  0.308541 -0.714694\n",
      "  -1.546564 -1.201903 -0.963083 -0.758956]\n",
      " [ 0.690061  0.985804 -0.670347 -0.95684  -0.956837  1.04101   0.46333\n",
      "  -0.516559 -0.43178  -0.018924  0.185335]]\n",
      "y_query:\n",
      " 1.0\n"
     ]
    }
   ],
   "source": [
    "#  x & y values in query set\n",
    "print(\"x_query:\\n\", test3_query[0][0])\n",
    "print(\"y_query:\\n\", test3_query[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f0d76e",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09b7e2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attention lstm\n",
    "class ALSTM(nn.Module):\n",
    "    def __init__(self,input_size:int, hidden_size:int,  num_layers:int):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=False)\n",
    "        self.lnorm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, x:torch.tensor, rt_attn=False):\n",
    "        #x: (W,L)\n",
    "        o,(h,_) = self.lstm(x) # o: (B,W,H) / h: (1,B,H)\n",
    "        print(o.shape, h.shape)\n",
    "        score = torch.mm(o,h.permute(1,0)) # (B,W,H)*(B,H,1) = (B,W,1)\n",
    "        tx_attn = F.softmax(score,dim=1) # (B,W)\n",
    "        print(tx_attn.shape)\n",
    "        context = torch.mm(torch.t(tx_attn),o) # (B,1,W)*(B,W,H) = (B,H)\n",
    "        normed_context = self.lnorm(context) # (B,H)\n",
    "        if rt_attn:\n",
    "            return normed_context, tx_attn\n",
    "        else:\n",
    "            return normed_context, None\n",
    "    \n",
    "# generating latent code z \n",
    "class LatentEmbedding(nn.Module):\n",
    "    def __init__(self,hidden_size:int):\n",
    "        super().__init__()\n",
    "        self.le = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 2*hidden_size, bias = False),\n",
    "             nn.ReLU(),\n",
    "             nn.Linear(2*hidden_size, 2*hidden_size, bias = False),\n",
    "             nn.ReLU())\n",
    "\n",
    "    def forward(self,x:torch.tensor):\n",
    "        out = self.le(x) # 2*hidden_size\n",
    "        return out\n",
    "        \n",
    "               \n",
    "class MetaStock(nn.Module):\n",
    "    # config : {'use_cuda': bool, 'input_size': int, 'hidden_size': int, 'num_layers': int, 'dropout': float, 'inner_l_rate_init': float,\n",
    "    # 'finetuning_lr_init': float}\n",
    "    def __init__(self,config):\n",
    "        super().__init__()\n",
    "        self._cuda = config['use_cuda']\n",
    "        self.input_size = config['input_size']\n",
    "        self.hidden_size = config['hidden_size']\n",
    "        self.num_layers = config['num_layers']\n",
    "        self.feature_transform = nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.encoder = ALSTM(self.hidden_size,self.hidden_size,self.num_layers)\n",
    "        self.latent_embedding = LatentEmbedding(self.hidden_size)\n",
    "        self.decoder = nn.Linear(self.hidden_size, 2*self.hidden_size, bias=False)\n",
    "        self.dropout = nn.Dropout(p=config['dropout'])\n",
    "        self.inner_l_rate = nn.Parameter(torch.FloatTensor([config['inner_l_rate_init']]))\n",
    "        self.finetuning_lr = nn.Parameter(torch.FloatTensor([config['finetuning_lr_init']]))\n",
    "        self.prob_layer = nn.LogSigmoid()\n",
    "        \n",
    "        \n",
    "    def encode(self,inputs, rt_attn):\n",
    "        # inputs -> [W,H]\n",
    "        inputs = self.dropout(inputs)\n",
    "        inputs = self.feature_transform(inputs)\n",
    "        encoded, attn = self.encoder(inputs, rt_attn)\n",
    "        h = self.latent_embedding(encoded)\n",
    "        latents = self.sample(h, self.hidden_size)\n",
    "        mean,var = h[:,:self.hidden_size], h[:,self.hidden_size:]\n",
    "        kl_div = self.cal_kl_div(latents, mean, var)\n",
    "        \n",
    "        return encoded, attn, latents, kl_div\n",
    "    \n",
    "    def cal_kl_div(self,latents, mean, var):\n",
    "        if self._cuda:\n",
    "            return torch.mean(self.cal_log_prob(latents, mean, var) - self.cal_log_prob(latents, torch.zeros(mean.size()).cuda(), torch.ones(var.size()).cuda()))\n",
    "        else:\n",
    "            return torch.mean(self.cal_log_prob(latents, mean, var) - self.cal_log_prob(latents, torch.zeros(mean.size()), torch.ones(var.size())))\n",
    "        \n",
    "    def cal_log_prob(self,x,mean,var):\n",
    "        eps = 1e-20\n",
    "        log_unnormalized = - 0.5 * ((x - mean)/ (var+eps))**2\n",
    "        log_normalization = torch.log(var+eps) + 0.5 * math.log(2*math.pi)\n",
    "\n",
    "        return log_unnormalized - log_normalization      \n",
    "    \n",
    "    def sample(self, weights, size):\n",
    "        mean, var = weights[:,:size], weights[:,size:]\n",
    "        z = torch.normal(0.0,1.0, mean.size()).cuda()\n",
    "        return mean+var*z\n",
    "    \n",
    "    def decode(self, latents):\n",
    "        weights = self.decoder(latents)\n",
    "        params = self.sample(weights,self.hidden_size)\n",
    "        return params\n",
    "    \n",
    "    def predict(self,encoded,params):\n",
    "        theta = params.view(-1, self.hidden_size, 1)\n",
    "        scores = encoded.unsqueeze(dim=1).bmm(theta).squeeze()\n",
    "        probs = torch.Tensor([self.prob_layer(scores)])\n",
    "        return probs\n",
    "    \n",
    "    def cal_accuracy(self, log_probs, target):\n",
    "        pred = (torch.exp(log_probs) >= 0.5).long()\n",
    "        correct = pred.eq(target).sum()\n",
    "        acc = correct / len(target)\n",
    "        return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48a9034",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1c79cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer():\n",
    "    def __init__(\n",
    "        self,\n",
    "        logging,\n",
    "        log_dir,\n",
    "        total_steps,\n",
    "        total_val_steps,\n",
    "        total_test_steps,\n",
    "        n_inner_step,\n",
    "        n_finetuning_step,\n",
    "        outer_lr,\n",
    "        verbose,\n",
    "        beta,\n",
    "        gamma,\n",
    "        lambda1,\n",
    "        lambda2,\n",
    "        clip_value,\n",
    "        load,\n",
    "        model_dir,\n",
    "        exp_name,\n",
    "        save_best,\n",
    "        save_checkpoint,\n",
    "        use_cuda,\n",
    "        n_tasks,\n",
    "        train,\n",
    "        valid_every_step,\n",
    "        print_every_step,\n",
    "        model_config,\n",
    "        data_config):\n",
    "        \n",
    "        self.model = MetaStock(model_config)\n",
    "        self.data = MetaStockDataset(data_config)\n",
    "        self._verbose = verbose\n",
    "        self._outer_lr = outer_lr\n",
    "        self._total_steps = total_steps\n",
    "        self._total_val_steps = total_val_steps\n",
    "        self._total_test_steps = total_test_steps\n",
    "        self._n_inner_step = n_inner_step\n",
    "        self._n_finetuning_step = n_finetuning_step\n",
    "        self._save_best = save_best\n",
    "        self._save_checkpoint = save_checkpoint\n",
    "        self._load_model = load\n",
    "        self._use_cuda = use_cuda\n",
    "        self._writer = SummaryWriter(log_dir)\n",
    "        self._loss_fn = nn.NLLLoss()\n",
    "        self._beta = beta # kl_weight\n",
    "        self._gamma = gamma # encoder_penalty_weight\n",
    "        self._lambda1 = lambda1 # l2-regularizer used in optimizer\n",
    "        self._lambda2 = lambda2 # orthogonality penalty weight\n",
    "        self._clip_value = clip_value,\n",
    "        self._valid_every_step = valid_every_step\n",
    "        self._print_every_step = print_every_step\n",
    "        self._logging = logging\n",
    "        self._n_tasks = n_tasks\n",
    "        \n",
    "        if train:\n",
    "            self.model_dir = os.path.join(model_dir,exp_name)\n",
    "            if not os.path.exists(self.model_dir):\n",
    "                os.makedirs(self.model_dir)\n",
    "        \n",
    "        if self._save_best:\n",
    "            self._best_acc = 0\n",
    "        \n",
    "        if self._use_cuda:\n",
    "            self.model = self.model.cuda()\n",
    "        \n",
    "        for p in self.model.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "                \n",
    "    def inner_loop(self,x_train,y_train):\n",
    "        \n",
    "        encoded, _, latents, kl_div = self.model.encode(x_train, rt_attn = False)\n",
    "        latents_init = latents\n",
    "        for i in range(self._n_inner_step):\n",
    "            latents.retain_grad()\n",
    "            params = self.model.decode(latents)\n",
    "            params.retain_grad()\n",
    "            probs = self.model.predict(encoded, params)\n",
    "            print(probs)\n",
    "            print(y_train)\n",
    "            train_loss = self._loss_fn(probs,y_train).requires_grad_(True)\n",
    "            print(\"train_loss:\", train_loss)\n",
    "            train_loss.backward(retain_graph=True)\n",
    "            print(latents.grad_fn)\n",
    "            latents = latents - self.model.inner_l_rate*latents.grad\n",
    "        encoder_penalty = torch.mean((latents_init-latents)**2)\n",
    "        return latents, kl_div, encoder_penalty\n",
    "    \n",
    "    def inner_finetuning(self, latents, x_train, y_train, x_val, y_val, verbose, step):\n",
    "#         x_train, y_train = support_data[0], support_data[1]\n",
    "#         x_val, y_val = query_data[0], query_data[1]\n",
    "        params = self.model.decode(latents)\n",
    "        params.retain_grad()\n",
    "        train_loss = self._loss_fn(probs,y_train)\n",
    "        train_acc = model.cal_accuracy(probs,y_train)\n",
    "        \n",
    "        # print info and logging\n",
    "        if verbose and step % self._print_every_step == 0:\n",
    "            print()\n",
    "            print('(Meta-Train) [Step: %d/%d] Train Loss: %4.4f Train Accuracy: %4.4f Inner_Lr: %4.4f Finetuning_Lr: %4.4f ' \\\n",
    "                   %(step, self._total_steps, train_loss.item(), train_acc.item(), self.model.inner_l_rate, self.model.finetuning_lr))\n",
    "        # logging\n",
    "        if self._logging and step%self._print_every_step == 0:\n",
    "            self.writer.add_scalar('Training Loss', train_loss.item(), step=step)\n",
    "            self.writer.add_scalar('Training Accuracy', train_acc.item(), step=step)\n",
    "            self.writer.add_scalar('Inner LR', float(model.inner_l_rate), step=step)\n",
    "            self.writer.add_scalar('Finetuning LR', float(model.inner_l_rate), step=step)\n",
    "\n",
    "        for j in range(self._n_finetuning_step):\n",
    "            train_loss.backward(retain_graph=True)\n",
    "            params = params - self.model.finetuning_lr * params.grad\n",
    "            params.retain_grad()\n",
    "            probs = model.predict(encoded, parameters)\n",
    "            train_loss = self._loss_fn(probs, y_train)\n",
    "            \n",
    "       # validation  -> x_val이 encoder 부분 통과해야함   \n",
    "    \n",
    "        val_encoded, _ = model.encode(x_val, rt_attn=False)\n",
    "        val_probs = model.predict(val_encoded, params)\n",
    "        val_loss = self._loss_fn(val_probs,y_val)\n",
    "        val_acc = model.cal_accuracy(val_probs, y_val) \n",
    "        \n",
    "        return val_loss, val_acc\n",
    "    \n",
    "    def orthogonality(self,params):\n",
    "        p2 = torch.mm(params,params.transpose(0,1))\n",
    "        p_norm = torch.norm(params, dim=1, keepdim=True) + 1e-20\n",
    "        correlation_matrix = w2 / torch.mm(p_norm, p_norm.transpose(0,1))\n",
    "        I = torch.eye(correlation_matrix.size(0)).cuda()\n",
    "        orthogonal_penalty = torch.mean((correlation_matrix-I)**2)\n",
    "        return orthogonal_penalty\n",
    "    \n",
    "    def meta_train(self,x_train,y_train,x_val,y_val, step, train=True):\n",
    "        # do task-train (inner loop)\n",
    "        latents, kl_div, encoder_penalty = self.inner_loop(x_train,y_train)\n",
    "        \n",
    "        # do inner fine-tuning & task-validate (outer loop)\n",
    "        val_loss, val_acc = self.inner_finetuning(latents, support_data,query_data, self._verbose and train, step)\n",
    "        orthogonality_penalty = self.orthogonality(list(self.model.decoder.parameters())[0])\n",
    "        \n",
    "        # calculate loss (l2 reg implemented with optimizer)\n",
    "        total_loss = val_loss + self._beta*kl_div + self._gamma*encoder_penalty + self._lambda2*orthogonality_penalty\n",
    "        \n",
    "        return total_loss, val_acc, kl_div, encoder_penalty, orthogonality_penalty\n",
    "    \n",
    "    def train(self):\n",
    "        # different optim for lr and params (only l2 penalize on params)\n",
    "        lr_list = ['inner_l_rate', 'finetuning_lr']\n",
    "        params = [x[1] for x in list(filter(lambda k: k[0] not in lr_list, self.model.named_parameters()))]\n",
    "        lr_params = [x[1] for x in list(filter(lambda k: k[0] in lr_list, self.model.named_parameters()))]\n",
    "        optim = torch.optim.Adam(params, lr=self._outer_lr, weight_decay = self._lambda1)\n",
    "        optim_lr = torch.optim.Adam(lr_params, lr=self._outer_lr)\n",
    "        \n",
    "        # update for (total steps) steps\n",
    "        for step in range(self._total_steps):\n",
    "            optim.zero_grad()\n",
    "            optim_lr.zero_grad()\n",
    "            #do training\n",
    "            \n",
    "            stock, support_idx, support, query_idx, query = self.data.task_formation(self._n_tasks)\n",
    "            support_x, support_y = support[0], support[1]\n",
    "            query_x, query_y = query[0], query[1]\n",
    "            for i in range(4*self._n_tasks):\n",
    "                \n",
    "                x_train = torch.FloatTensor(support_x[i]).to(device='cuda:0') \n",
    "                y_train = torch.LongTensor([support_y[i]])\n",
    "#                 y_train = torch.FloatTensor([support_y[i]]).to(device='cuda:0')\n",
    "                x_val =  torch.FloatTensor(query_x[i]).to(device='cuda:0')\n",
    "                y_val = torch.LongTensor([query_y[i]])\n",
    "#                 y_val = torch.FloatTensor([query_y[i]]).to(device='cuda:0')\n",
    "                print(x_train)\n",
    "                print(y_train)\n",
    "                print(x_val)\n",
    "                print(y_val)\n",
    "                total_loss, val_acc, encoder_penalty, orthogonality_penalty = self.meta_train(x_train,y_train,x_val,y_val, step, train=True)\n",
    "\n",
    "                if self._verbose and step % self._print_every_step == 0:\n",
    "                    print('(Meta-Valid) [Step: %d/%d] Total Loss: %4.4f Valid Accuracy: %4.4f'%(step, self._total_steps, val_loss.item(), val_acc.item()))\n",
    "                    print('(Meta-Valid) [Step: %d/%d] KL: %4.4f Encoder Penalty: %4.4f Orthogonality Penalty: %4.4f'%(step, self._total_steps, kl_div, encoder_penalty, orthogonality_penalty))\n",
    "\n",
    "                total_loss.backward()\n",
    "                nn.utils.clip_grad_value_(self.model.parameters(),self._clip_value)\n",
    "                nn.utils.clip_grad_norm_(self.model.parameters(), self._clip_value)\n",
    "                optim.step()\n",
    "                optim_lr.step()\n",
    "\n",
    "                if step % self._valid_every_step == 1:\n",
    "                    self.model.eval()\n",
    "                    val_losses = []\n",
    "                    val_accs = []\n",
    "\n",
    "                    for val_step in range(self.total_val_steps):\n",
    "                        val_loss, val_acc, _, _, _ = self.meta_train(x_train,y_train,x_val,y_val, step, train=False)\n",
    "                        val_losses.append(val_loss.item())\n",
    "                        val_accs.append(val_acc.item())\n",
    "\n",
    "                    if self._save_checkpoint:\n",
    "                        #save checkpoint                      \n",
    "                        if not (self._save_best and sum(val_accs)/len(val_accs) < self._best_acc):\n",
    "                            model_name = '%dk_%4.4f_%4.4f_model.pth' % (step//1000, sum(val_losses)/len(val_losses), sum(val_accs)/len(val_accs))\n",
    "                            state = {'step': step, 'val_acc': sum(val_accs)/len(val_accs), 'state_dict': self.model.state_dict()}\n",
    "                            if not os.path.exists(self.model_dir):\n",
    "                                os.mkdir(self.model_dir)\n",
    "                            torch.save(state, os.path.join(self.model_dir, model_name))\n",
    "\n",
    "                    self.model.train()   \n",
    "                    if self._verbose:\n",
    "                        print()\n",
    "                        print('=' * 50)\n",
    "                        print('Meta Valid Loss: %4.4f \\nMeta Valid Accuracy: %4.4f'%(sum(val_losses)/len(val_losses), sum(val_accs)/len(val_accs)))\n",
    "                        print('=' * 50)\n",
    "                        print()\n",
    "                        if self._save_checkpoint:\n",
    "                            print('Saving checkpoint %s...'%model_name)\n",
    "                            print()\n",
    "                        \n",
    "    def test(self):\n",
    "        \n",
    "\n",
    "        #load state dict\n",
    "        state_dict = torch.load(self._load_model)['state_dict']\n",
    "        self.model.load_state_dict(state_dict)\n",
    "\n",
    "        self.model.eval()\n",
    "        test_losses = []\n",
    "        test_accs = []\n",
    "        \n",
    "        for test_step in range(self._total_test_steps):\n",
    "            stock, support_idx, support_data, query_idx, query_data = self.data.task_formation(self._n_tasks)\n",
    "            test_loss, test_acc, _, _, _ = self.run_batch(suport_data, query_data, test_step, False)\n",
    "            test_losses.append(test_loss.item())\n",
    "            test_accs.append(test_acc.item())\n",
    "\n",
    "        if self._verbose:\n",
    "            print()\n",
    "            print('=' * 50)\n",
    "            print('Meta Test Loss: %4.4f Meta Test Accuracy: %4.4f'%(sum(test_losses)/len(test_losses), sum(test_accs)/len(test_accs)))\n",
    "            print('=' * 50)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54d0de30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config =  {'no_cuda': False, 'data_path': '/home/yjhwang/stock_prediction/data/stocknet-dataset/price/ourpped', 'verbose': 0, 'seed': 300, 'train_stock_num':67,\n",
    "'train_date': '2014-01-02', 'test_date': '2015-08-03', 'train':'train', 'train_stock': []}\n",
    "model_config = {'use_cuda': True, 'input_size': 11, 'hidden_size': 32, 'num_layers': 1, 'dropout': 0.3, 'inner_l_rate_init': 1,\n",
    "    'finetuning_lr_init': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86112b0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6a330278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7210e-01,  4.7941e-01, -1.7210e-01,  1.6494e+00,  1.6494e+00,\n",
      "         -3.4420e-02,  8.0762e-01,  2.1955e+00,  2.9607e+00,  4.1711e+00,\n",
      "          4.0746e+00],\n",
      "        [-4.9168e-02,  7.9912e-01, -7.3764e-01, -1.2295e-02, -1.2298e-02,\n",
      "         -5.6798e-01,  7.1675e-01,  1.9974e+00,  2.6857e+00,  3.7507e+00,\n",
      "          4.0419e+00],\n",
      "        [-4.0010e-01,  6.0616e-02, -1.0063e+00,  1.4015e+00,  1.4016e+00,\n",
      "         -1.5762e+00, -6.4624e-01,  4.5180e-01,  9.8082e-01,  1.9912e+00,\n",
      "          2.6495e+00],\n",
      "        [-2.7060e-01,  8.4870e-01, -3.5671e-01, -1.4306e+00, -1.4307e+00,\n",
      "         -2.0000e-06,  5.0061e-01,  1.5055e+00,  2.1402e+00,  3.0534e+00,\n",
      "          4.1103e+00],\n",
      "        [-8.7623e-01,  3.4076e-01, -8.8840e-01,  1.0701e+00,  1.0701e+00,\n",
      "         -5.3790e-01, -6.9612e-01,  1.0872e-01,  9.1092e-01,  1.7388e+00,\n",
      "          2.8652e+00]], device='cuda:0')\n",
      "tensor([0])\n",
      "tensor([[-1.7210e-01,  4.7941e-01, -1.7210e-01,  1.6494e+00,  1.6494e+00,\n",
      "         -3.4420e-02,  8.0762e-01,  2.1955e+00,  2.9607e+00,  4.1711e+00,\n",
      "          4.0746e+00],\n",
      "        [-4.9168e-02,  7.9912e-01, -7.3764e-01, -1.2295e-02, -1.2298e-02,\n",
      "         -5.6798e-01,  7.1675e-01,  1.9974e+00,  2.6857e+00,  3.7507e+00,\n",
      "          4.0419e+00],\n",
      "        [-4.0010e-01,  6.0616e-02, -1.0063e+00,  1.4015e+00,  1.4016e+00,\n",
      "         -1.5762e+00, -6.4624e-01,  4.5180e-01,  9.8082e-01,  1.9912e+00,\n",
      "          2.6495e+00],\n",
      "        [-2.7060e-01,  8.4870e-01, -3.5671e-01, -1.4306e+00, -1.4307e+00,\n",
      "         -2.0000e-06,  5.0061e-01,  1.5055e+00,  2.1402e+00,  3.0534e+00,\n",
      "          4.1103e+00],\n",
      "        [-8.7623e-01,  3.4076e-01, -8.8840e-01,  1.0701e+00,  1.0701e+00,\n",
      "         -5.3790e-01, -6.9612e-01,  1.0872e-01,  9.1092e-01,  1.7388e+00,\n",
      "          2.8652e+00],\n",
      "        [ 1.4388e+00,  1.4388e+00, -2.1268e-01, -2.7261e+00, -2.7261e+00,\n",
      "          1.8942e+00,  1.8178e+00,  2.3637e+00,  3.4818e+00,  4.2107e+00,\n",
      "          5.3338e+00]], device='cuda:0')\n",
      "tensor([0])\n",
      "torch.Size([5, 32]) torch.Size([1, 32])\n",
      "torch.Size([5, 1])\n",
      "tensor([-0.4080])\n",
      "tensor([0])\n",
      "train_loss: tensor(0.4080, requires_grad=True)\n",
      "<AddBackward0 object at 0x7f7830321e50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39111/1399484577.py:167: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y_train = torch.LongTensor([support_y[i]])\n",
      "/tmp/ipykernel_39111/1399484577.py:170: DeprecationWarning: an integer is required (got type numpy.float64).  Implicit conversion to integers using __int__ is deprecated, and may be removed in a future version of Python.\n",
      "  y_val = torch.LongTensor([query_y[i]])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mModelTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./log\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_val_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_test_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_inner_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_finetuning_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mouter_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0e-9\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0e-8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlambda2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclip_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mload\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./train_model/model.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./train_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexample\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_best\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m        \u001b[49m\u001b[43msave_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cuda\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_tasks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalid_every_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_every_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_config\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28mprint\u001b[39m(x_val)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_val)\n\u001b[0;32m--> 176\u001b[0m total_loss, val_acc, encoder_penalty, orthogonality_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmeta_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;129;01mand\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_print_every_step \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(Meta-Valid) [Step: \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m] Total Loss: \u001b[39m\u001b[38;5;132;01m%4.4f\u001b[39;00m\u001b[38;5;124m Valid Accuracy: \u001b[39m\u001b[38;5;132;01m%4.4f\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m%\u001b[39m(step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_total_steps, val_loss\u001b[38;5;241m.\u001b[39mitem(), val_acc\u001b[38;5;241m.\u001b[39mitem()))\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mModelTrainer.meta_train\u001b[0;34m(self, x_train, y_train, x_val, y_val, step, train)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmeta_train\u001b[39m(\u001b[38;5;28mself\u001b[39m,x_train,y_train,x_val,y_val, step, train\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# do task-train (inner loop)\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m     latents, kl_div, encoder_penalty \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# do inner fine-tuning & task-validate (outer loop)\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_finetuning(latents, support_data,query_data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_verbose \u001b[38;5;129;01mand\u001b[39;00m train, step)\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mModelTrainer.inner_loop\u001b[0;34m(self, x_train, y_train)\u001b[0m\n\u001b[1;32m     84\u001b[0m     train_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mprint\u001b[39m(latents\u001b[38;5;241m.\u001b[39mgrad_fn)\n\u001b[0;32m---> 86\u001b[0m     latents \u001b[38;5;241m=\u001b[39m latents \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39minner_l_rate\u001b[38;5;241m*\u001b[39m\u001b[43mlatents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\n\u001b[1;32m     87\u001b[0m encoder_penalty \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean((latents_init\u001b[38;5;241m-\u001b[39mlatents)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m latents, kl_div, encoder_penalty\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'data'"
     ]
    }
   ],
   "source": [
    "ModelTrainer(\n",
    "        logging=True,\n",
    "        log_dir=\"./log\",\n",
    "        total_steps=10,\n",
    "        total_val_steps=10,\n",
    "        total_test_steps=10,\n",
    "        n_inner_step=10,\n",
    "        n_finetuning_step=10,\n",
    "        outer_lr=0.0001,\n",
    "        verbose=True,\n",
    "        beta=0.001,\n",
    "        gamma=1.0e-9,\n",
    "        lambda1=1.0e-8,\n",
    "        lambda2=0.1,\n",
    "        clip_value=0.1,\n",
    "        load=\"./train_model/model.pth\",\n",
    "        model_dir=\"./train_model\",\n",
    "        exp_name=\"example\",\n",
    "        save_best=True,\n",
    "        save_checkpoint=True,\n",
    "        use_cuda=True,\n",
    "        n_tasks=50,\n",
    "        train=True,\n",
    "        valid_every_step=2,\n",
    "        print_every_step=2,\n",
    "        model_config=model_config,\n",
    "        data_config=data_config).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b4f1ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor([-2]).requires_grad_(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209f9494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "[python3:leo]",
   "language": "python",
   "name": "leo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
